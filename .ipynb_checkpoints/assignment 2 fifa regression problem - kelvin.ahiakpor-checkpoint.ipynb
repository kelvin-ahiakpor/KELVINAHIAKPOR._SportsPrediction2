{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "484417e2",
   "metadata": {},
   "source": [
    "# Assignment 2: FIFA Regression Problem - kelvin.ahiakpor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d927df5d",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a632fe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import streamlit as st\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV, KFold, train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from xgboost import XGBRegressor\n",
    "from joblib import Parallel, delayed\n",
    "from dill import dump\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f42456d",
   "metadata": {},
   "source": [
    "### Setting job timeout for computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dafd4d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['JOBLIB_START_METHOD'] = 'loky'\n",
    "os.environ['JOBLIB_TIMEOUT'] = '300'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb82dc6f",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ee2f132",
   "metadata": {},
   "outputs": [],
   "source": [
    "players = pd.read_csv('male_players (legacy).csv', low_memory=False)\n",
    "players_22 = pd.read_csv('players_22.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ad035b",
   "metadata": {},
   "source": [
    "### Understanding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc93fb20",
   "metadata": {},
   "source": [
    "##### Custom recipes for Data Loading and Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "288fa4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 3) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce60e3e",
   "metadata": {},
   "source": [
    "##### Peek at training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65d2c734",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dimensions of data set is {players.shape} This is a large dataset.\\n\")\n",
    "players.info()\n",
    "players.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c52357",
   "metadata": {},
   "source": [
    "##### Quick statistics\n",
    "Extended Five-number Summary,\n",
    "Histogram Plots,\n",
    "Correlation Matrix,\n",
    "Categorical Variable Selection &\n",
    "Strongly Correlated Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0e79f83-5fea-4d6a-9d81-c5cb0497f062",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extended Five-number Summary\")\n",
    "players.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08777a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "players.hist(bins=50, figsize=(23,20))\n",
    "print(\"Histogram Plots\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27370c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlation Matrix\")\n",
    "quant_players = players.select_dtypes(include=[np.int64, np.float64])\n",
    "corrMat = quant_players.corr()\n",
    "corrMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a1a1f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Categorical Variable Selection\")\n",
    "cat_players = players.select_dtypes(include=[object])\n",
    "cat_players = cat_players[[\"work_rate\"]]\n",
    "\n",
    "for column in cat_players.columns:\n",
    "    print(cat_players[column].value_counts())\n",
    "    print()\n",
    "    \n",
    "cat_players.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53efd65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most Correlated Numeric Features\")\n",
    "most_correlated = corrMat[\"overall\"].abs()[corrMat[\"overall\"].abs() > 0.50].sort_values(ascending=False)\n",
    "most_correlated_cols = most_correlated.index.tolist()\n",
    "most_correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb824de",
   "metadata": {},
   "source": [
    "##### Descriptions of the most correlated numeric features from Kaggle\n",
    "1. overall - player current overall attribute\n",
    "2. movement_reactions - player reactions attribute\n",
    "3. potential - player potential overall attribute\n",
    "4. mentality_composure - player composure attribute\n",
    "5. passing - player passing attribute\n",
    "6. wage_eur - player weekly wage (in eur)\n",
    "7. dribbling - player dribbling attribute\n",
    "8. release_clause_eur - player release clause (in eur) - if applicable\n",
    "9. value_eur - player value (in eur)\n",
    "10. physic - player physic attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607f49ff",
   "metadata": {},
   "source": [
    "#### Some notes so far\n",
    "\n",
    "1. Possible new features for feature engineering:   \n",
    "\n",
    "    **Transformation Features** \n",
    "    - log_value_eur - the histogram plots show player value (`value_eur`) is heavily right skewed. Taking the logarithm helps us normalize.\n",
    "    - log_wage_eur - the histogram plots show wages (`wage_eur`) is heavily right skewed. Taking the logarithm helps us normalize the distribution.\n",
    "    - log_release_clause_eur - the histogram plots show release clause (`release_clause_eur`) is heavily right skewed. Try normalizing with log.\n",
    "\n",
    "    **Interaction Features** \n",
    "    - passing_dribbling_interaction - helps us understand the combined effect of passing and dribbling on overall rating.\n",
    "\n",
    "    **Ratio Features**\n",
    "    - potential/value_eur ratio - a higher ratio may indicate that the player is undervalued relative to their potential (`potential`).\n",
    "    - physic/movement_reactions ratio - a balanced ratio suggests a well-rounded player, while extremes may highlight specialized roles or weaknesses.\n",
    "    - age vs potential ratio - with this ratio younger players are expected to have higher growth potential (`potential`). *\n",
    "    - wage vs value ratio - this ratio shows how the value of a player's contributions (`wage_eur`) compare to their market value (`value_eur`). *\n",
    "    - composure vs reactions ratio - better players make good decisions under pressure. A balanced ratio indicates a calm but swift player.\n",
    "    - passing vs dribbling ratio - a higher ratio suggests a player is more proficient in distributing the ball relative to individual ball-handling skills.\n",
    "\n",
    "    These ratios may (arguably*) influence overall player rating\n",
    "    * age vs potential ratio - some players have an older peak age\n",
    "    * wage vs value ratio - older players on higher wages may still have lower value \n",
    "    \n",
    "    For transformation, we could use exponentiating if a left-skewed distributed was observed\n",
    "    \n",
    "\n",
    "2. Imputation:\n",
    "   - Since not every player has a release clause, when imputing for N/As in release clause, simply use `value_eur`.\n",
    "   - `value_eur` is the best estimation of the player's current or most recent valuation.\n",
    "   - Use dataframe joining methods after removing duplicates. Concatenating will defeat the purpose of duplicate removal and add the duplicate rows back.\n",
    "\n",
    "3. Evaluation:\n",
    "   - Since the dataset is large (150,000+ rows) use at least 5-fold cross-validation.\n",
    "   - Spot checking?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82755f5-c6f8-4d0b-8cef-accc84a91e84",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "Demonstrate the data preparation & feature extraction process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbc50e3",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b6212c",
   "metadata": {},
   "source": [
    "##### From the quick statistics above, we identified the most relevant numeric columns. Now we pick only those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "249f9136",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = players[most_correlated_cols]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5ae2339",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4968886",
   "metadata": {},
   "source": [
    "##### Duplicates arise because some rows have the same values in these the 10 columns even though the original dataset has no duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "115686fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f64c19",
   "metadata": {},
   "source": [
    "##### Imputing missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c1fc947",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['release_clause_eur'].isnull(), 'release_clause_eur'] = df['value_eur']\n",
    "#uses boolean series as rows, release clause as column. \n",
    "#each null release clause will now have the value of it's respective player's value\n",
    "\n",
    "imputer_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean'))\n",
    "])\n",
    "\n",
    "with open(\"./imputer.pkl\", 'wb') as f:\n",
    "    pkl.dump(imputer_pipeline, f)\n",
    "    \n",
    "df = pd.DataFrame(imputer_pipeline.fit_transform(df), columns=df.columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2df10d",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2580eafa",
   "metadata": {},
   "source": [
    "##### Encoding categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e6f54cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_pipeline = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', OneHotEncoder(sparse_output=False),cat_players.columns)\n",
    "])\n",
    "\n",
    "cat_players_encoded = encoding_pipeline.fit_transform(cat_players)\n",
    "\n",
    "df2 = pd.DataFrame(cat_players_encoded, columns=encoding_pipeline.get_feature_names_out())\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ffbc40",
   "metadata": {},
   "source": [
    "##### Merging the numeric and categorical data\n",
    "We remain cautious of the dataframe dimensions. \n",
    "\n",
    "Since we removed duplicates in numeric dataframe we must maintain that row count in the merged dataframe for training. \n",
    "\n",
    "We join on unique column player_id which we will retrieve from players dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f703fa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07354985",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9982682a",
   "metadata": {},
   "source": [
    "We have to add the player_id to our dataframes first.\n",
    "Below we retrieve unique player_id from players into numeric and cateogrical dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba92ffaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'player_id'] = players.loc[df.index, 'player_id'].values\n",
    "df2.loc[:, 'player_id'] = players.loc[df2.index, 'player_id'].values\n",
    "#The code below checks if the df has the correct ids from players even though we removed the duplicates initially\n",
    "#didn't want a case where the id between the cleaned df mismatch the ids in the players\n",
    "#The duplicates indices were retrieved from running this code: df[df.duplicated()]. \n",
    "#Some duplicate indicies to test [2904,161365,3007]\n",
    "\n",
    "# print(players.iloc[161365]['player_id'])\n",
    "# print(df.iloc[161365]['player_id'])\n",
    "\n",
    "# print(players.iloc[161365]['player_id'])\n",
    "# print(df.iloc[161365]['player_id'])\n",
    "#Successful!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a27704a",
   "metadata": {},
   "source": [
    "Finally, we merge!\n",
    "... and drop player_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4da2dccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "playersNeeded = pd.merge(df, df2, on='player_id')\n",
    "playersNeeded.drop(columns=['player_id'], inplace=True)\n",
    "playersNeeded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4081c1-35db-49b4-a354-3edd7d2b54f6",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "Create feature subsets that show maximum correlation with the dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6506ea76",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d7014a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfP = playersNeeded\n",
    "#Transformed Features\n",
    "dfP['log_wage_eur'] = np.log(dfP['wage_eur'])\n",
    "dfP['log_value_eur'] = np.log(dfP['value_eur'])\n",
    "dfP['log_release_clause_eur'] = np.log(dfP['release_clause_eur'])\n",
    "#Interaction Features\n",
    "dfP['passing_dribbling_interaction'] = dfP['passing'] * dfP['dribbling']\n",
    "#Ratio Features\n",
    "dfP['potential_value_ratio'] = dfP['potential'] / dfP['value_eur']\n",
    "dfP['physic_movement_reactions_ratio'] = dfP['physic'] / dfP['movement_reactions']\n",
    "dfP['passing_dribbling_ratio'] = dfP['passing'] / dfP['dribbling']\n",
    "dfP['wage_value_ratio'] = dfP['wage_eur'] / dfP['value_eur']\n",
    "dfP['composure_reactions_ratio'] = dfP['mentality_composure'] / dfP['movement_reactions']\n",
    "dfP.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f01a0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most Correlated Features\")\n",
    "corrMat = dfP.corr()\n",
    "most_correlated = corrMat[\"overall\"].abs()[corrMat[\"overall\"].abs() > 0.60].sort_values(ascending=False)\n",
    "most_correlated_cols = most_correlated.index.tolist()\n",
    "most_correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca030d3b",
   "metadata": {},
   "source": [
    "The transformed features in our feature engineering came out with higher correlations as compared to the ratio features. \n",
    "\n",
    "The passing_dribbling_interaction also proved useful. \n",
    "\n",
    "Now we will move on, selecting only these 9 features and 1 label to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae1a7321",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfP = dfP[most_correlated_cols]\n",
    "dfP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe5a6fa",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dade66a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfP.drop('overall',axis=1)\n",
    "y = dfP['overall']\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "X_scaled = pipeline.fit_transform(X)\n",
    "X_scaled\n",
    "\n",
    "with open(\"./scaler.pkl\", 'wb') as f:\n",
    "    pkl.dump(pipeline, f)\n",
    "    \n",
    "X = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16872a5b-ee7b-4143-967b-b1ce29aa5444",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "Create and train a suitable machine learning model with cross-validation that can predict a player's rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2e9ce8",
   "metadata": {},
   "source": [
    "We will fit and train 3 models with cross validation and select the best one. \n",
    "\n",
    "Metrics are models are defined in dictionaries with their inbuilt scorer names\n",
    "\n",
    "In our cross validation, we will shuffle the dataset since it is currently ordered from best to worst player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af74f628",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'RandomForest': RandomForestRegressor(random_state=8),\n",
    "    'XGBoost': XGBRegressor(random_state=8),\n",
    "    'GradientBoost': GradientBoostingRegressor(random_state=8)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5a7f6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c932a6bc",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95ac49ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, random_state=8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d901aca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=3, random_state=8, shuffle=True)\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train,y_train)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=kfold, scoring='r2',n_jobs=-1)\n",
    "    #dump(model, open(\"./models/Fifa\"+model.__class__.__name__ + \".pkl\", mode=\"wb\"))\n",
    "    #split(\"./models/Fifa\"+model.__class__.__name__ + \".pkl\", \"./models/\", int(5E7))\n",
    "    predictions[name] = model.predict(X_test)\n",
    "    print(f\"\\n{name} Cross-Validation Scores:\")\n",
    "    print(scores)\n",
    "    print(f\"R2 Avg (sd): {scores.mean() * 100.0:.3f}% ({scores.std() * 100.0:.3f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8783bf35-f6bf-4a91-bb4f-fc2563e39163",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "Measure the model's performance and fine-tune it as a process of optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf5bfcf",
   "metadata": {},
   "source": [
    "We will use RMSE to evaluate because it is an interpretable metric in same units as target variable (player rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4bf67e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    'RMSE': 'neg_root_mean_squared_error',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c55239",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8d87e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models.items():\n",
    "    print(f\"Evaluation for {name}:\")\n",
    "    rmse = np.sqrt(mean_squared_error(y_test,predictions[model]))\n",
    "    print(f\"  RMSE: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577ebe5a",
   "metadata": {},
   "source": [
    "Looking at both goodness of fit (R2) and RMSE we see that the Random Forest Regressor is the best model so we will fine tune and use it moving forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eca420d",
   "metadata": {},
   "source": [
    "### Fine-tuning\n",
    "During fine tuning, re-testing and re-evaluation is done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d192e080",
   "metadata": {},
   "source": [
    "Creating parameter dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813e7290",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    'RandomForest': {\n",
    "        'n_estimators': randint(50, 200),\n",
    "        'max_features': [1.0, 'sqrt', 'log2'],\n",
    "        'max_depth': [None, 10, 20, 30]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94d0934",
   "metadata": {},
   "source": [
    "Tuning Random Forest with Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b789d048",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = models['RandomForest']\n",
    "param_grid = param_grids['RandomForest']\n",
    "rsearch = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=7, \n",
    "                             scoring='neg_root_mean_squared_error', cv=kfold, random_state=8, n_jobs=-1)\n",
    "rsearch.fit(X_train, y_train)\n",
    "best_model = rsearch.best_estimator_\n",
    "print(f\"\\n{name} Best Parameters: {rsearch.best_params_}\")\n",
    "print(f\"Best RMSE: {-rsearch.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efa77c7",
   "metadata": {},
   "source": [
    "### Exporting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b94842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(src, dest, wsize):\n",
    "    src = Path(src)\n",
    "    dest = Path(dest)\n",
    "    dest.mkdir(exist_ok=True)\n",
    "\n",
    "    partNum = 0\n",
    "    with open(src, \"rb\") as f:\n",
    "        while True:\n",
    "            chunk = f.read(wsize)\n",
    "\n",
    "            if not chunk:\n",
    "                break\n",
    "\n",
    "            partNum += 1\n",
    "            filename = f\"{src.stem}-{partNum}.pkl\"\n",
    "            with open(dest / filename, \"wb\") as p:\n",
    "                p.write(chunk)\n",
    "    src.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adce9f02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dump(best_model, open(\"./models/Fifa\"+model.__class__.__name__ + \".pkl\", mode=\"wb\"))\n",
    "split(\"./models/Fifa\"+model.__class__.__name__ + \".pkl\", \"./models/\", int(5E7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbba20d-6f41-46b7-a018-9c635019d34d",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "Use the data from another season (players_22) which was not used during the training to test how good is the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4ce505",
   "metadata": {},
   "source": [
    "Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30be8727",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['overall', 'movement_reactions', 'potential', 'wage_eur', 'release_clause_eur', 'value_eur', 'passing', 'dribbling']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e5fa39",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46b9571",
   "metadata": {},
   "source": [
    "Create function that automates the pipeline\n",
    "- Impute\n",
    "- Feature Engineer\n",
    "- Scale\n",
    "- Fit\n",
    "- Predict\n",
    "- Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4893c0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_model(players_data):\n",
    "    # select features\n",
    "    players_22 = players_data[features].copy()  # Ensure original data is not modified\n",
    "\n",
    "    # identify columns with NaN values\n",
    "    nan_columns = players_22.columns[players_22.isna().any()].tolist()\n",
    "\n",
    "    # impute NaN values with mean\n",
    "    imputation_dict = {col: players_22[col].mean() for col in nan_columns}\n",
    "    players_22.fillna(imputation_dict, inplace=True)\n",
    "\n",
    "    # feature engineering\n",
    "    players_22['log_wage_eur'] = np.log(players_22['wage_eur'])\n",
    "    players_22['log_value_eur'] = np.log(players_22['value_eur'])\n",
    "    players_22['log_release_clause_eur'] = np.log(players_22['release_clause_eur'])\n",
    "    players_22['passing_dribbling_interaction'] = players_22['passing'] * players_22['dribbling']\n",
    "\n",
    "    selected_features = ['overall','log_value_eur', 'log_release_clause_eur', 'movement_reactions', 'log_wage_eur', 'potential', 'wage_eur', 'passing_dribbling_interaction', 'value_eur', 'passing']\n",
    "    players_22 = players_22[selected_features]\n",
    "    \n",
    "    # separate features and label\n",
    "    X = players_22.drop('overall', axis=1)\n",
    "    y = players_22['overall']\n",
    "\n",
    "    # scale the features using StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # impute any remaining NaNs after feature engineering\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_final = imputer.fit_transform(X_scaled)\n",
    "\n",
    "    # load and predict with the trained model\n",
    "    with open(\"./models/FifaRandomForestRegressor.pkl\", \"rb\") as model_file:\n",
    "        model = pkl.load(model_file)\n",
    "\n",
    "    predictions = model.predict(X_final)\n",
    "\n",
    "    # evaluate\n",
    "    rmse = np.sqrt(mean_squared_error(y, predictions))\n",
    "    r2_value = r2_score(y, predictions)\n",
    "\n",
    "    print(\"Evaluation scores of RandomForestRegressor on players dataset\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"R2: {r2_value*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d9dcb4",
   "metadata": {},
   "source": [
    "Call the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4fc51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_and_model(players_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110233d8-276f-4a21-a0b1-cf5111b6e4a3",
   "metadata": {},
   "source": [
    "# Question 6\n",
    "Deploy the model on a simple web page using either (Heroku, Streamlite, or Flask) and upload a video that shows how the model performs on the web page/site."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe1bb5c",
   "metadata": {},
   "source": [
    "### Webpage Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47eecec5",
   "metadata": {},
   "source": [
    "The code below creates the webpage. It has been moved to a .py file to be run on a separate streamlit host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4b2289",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import streamlit as st\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import pickle as pkl\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.impute import SimpleImputer\n",
    "\n",
    "# # Load the trained model\n",
    "# with open(\"/Users/kelvin/Library/CloudStorage/OneDrive-AshesiUniversity/ASHESI/2ND YEAR/SEM 2/AI/assignments/assignment 2/models/FifaRandomForestRegressor.pkl\", \"rb\") as model_file:\n",
    "#     model = pkl.load(model_file)\n",
    "\n",
    "# # # Load the imputer\n",
    "# # with open(\"./imputer.pkl\", \"rb\") as imputer_file:\n",
    "# #     imputer = pkl.load(imputer_file)\n",
    "\n",
    "# # Load the scaler\n",
    "# with open(\"/Users/kelvin/Library/CloudStorage/OneDrive-AshesiUniversity/ASHESI/2ND YEAR/SEM 2/AI/assignments/assignment 2/scaler.pkl\", \"rb\") as scaler_file:\n",
    "#     scaler = pkl.load(scaler_file)\n",
    "\n",
    "# # Function to preprocess input data and predict using the trained model\n",
    "# def predict_overall(player_data):\n",
    "#     # Convert input data into a DataFrame\n",
    "#     player_features = pd.DataFrame(player_data, index=[0])\n",
    "\n",
    "#     # Handle the case where release_clause_eur is zero by setting it to value_eur\n",
    "#     player_features.loc[player_features['release_clause_eur'] == 0, 'release_clause_eur'] = player_features['value_eur']\n",
    "\n",
    "#     # Apply logarithm transformations\n",
    "#     player_features['log_wage_eur'] = np.log(player_features['wage_eur'] + 1)\n",
    "#     player_features['log_value_eur'] = np.log(player_features['value_eur'] + 1)\n",
    "#     player_features['log_release_clause_eur'] = np.log(player_features['release_clause_eur'] + 1)\n",
    "#     player_features['passing_dribbling_interaction'] = player_features['passing'] * player_features['dribbling']\n",
    "\n",
    "#     selected_features = ['log_value_eur', 'log_release_clause_eur', 'movement_reactions', 'log_wage_eur', 'potential', 'wage_eur', 'passing_dribbling_interaction', 'value_eur', 'passing']\n",
    "#     player_features = player_features[selected_features]\n",
    "\n",
    "# #     # Impute missing values using the loaded imputer\n",
    "# #     player_features_imputed = imputer.transform(player_features)\n",
    "\n",
    "#     # Scale the input data using the loaded scaler\n",
    "#     X_scaled = scaler.transform(player_features)\n",
    "\n",
    "#     # Make predictions\n",
    "#     predicted_rating = model.predict(X_scaled)\n",
    "\n",
    "#     # For demonstration purpose, assuming a fixed margin of error (you can adjust this)\n",
    "#     margin_of_error = 1.96 * 0.46  # Adjust based on your desired confidence level and RMSE\n",
    "\n",
    "#     lower_bound = predicted_rating - margin_of_error\n",
    "#     upper_bound = predicted_rating + margin_of_error\n",
    "\n",
    "#     return predicted_rating[0], lower_bound[0], upper_bound[0]\n",
    "\n",
    "# # Streamlit UI\n",
    "# st.title('FIFA Player Rating Predictor')\n",
    "\n",
    "# # Sidebar with input fields for features\n",
    "# movement_reactions = st.number_input('Enter Movement Reactions', min_value=0, step=1)\n",
    "# potential = st.number_input('Enter Potential', min_value=0, step=1)\n",
    "# wage_eur = st.number_input('Enter Wage (EUR)', min_value=0, step=1)\n",
    "# release_clause_eur = st.number_input('Enter Release Clause (EUR)', min_value=0, step=1)\n",
    "# value_eur = st.number_input('Enter Value (EUR)', min_value=0, step=1)\n",
    "# passing = st.number_input('Enter Passing', min_value=0, step=1)\n",
    "# dribbling = st.number_input('Enter Dribbling', min_value=0, step=1)\n",
    "\n",
    "# # Prepare player data for prediction\n",
    "# player_data = {\n",
    "#     'movement_reactions': movement_reactions,\n",
    "#     'potential': potential,\n",
    "#     'wage_eur': wage_eur,\n",
    "#     'release_clause_eur': release_clause_eur,\n",
    "#     'value_eur': value_eur,\n",
    "#     'passing': passing,\n",
    "#     'dribbling': dribbling\n",
    "# }\n",
    "\n",
    "# # Predict button\n",
    "# if st.button('Predict Player Rating'):\n",
    "#     # Make prediction\n",
    "#     predicted_rating, lower_bound, upper_bound = predict_overall(player_data)\n",
    "\n",
    "#     # Display results\n",
    "#     st.success(f'Predicted Overall Rating: {predicted_rating:.2f}')\n",
    "#     st.info(f'Confidence Interval: [{lower_bound:.2f}, {upper_bound:.2f}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d64ce3",
   "metadata": {},
   "source": [
    "### Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa603c9",
   "metadata": {},
   "source": [
    "### Video Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4544586",
   "metadata": {},
   "source": [
    "The plan for the video is as follows\n",
    "- Find new data that our model has never seen\n",
    "- Select data from the top, middle and end (high-low rating)\n",
    "- Select players without release clause\n",
    "- Concatenate into dataframe for predictions\n",
    "- Show predictions for a high, medium, low rated and no release clause player"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddde133",
   "metadata": {},
   "source": [
    "#### Loading completely new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a6654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_19 = pd.read_csv('players_19.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18e7cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['short_name','overall', 'movement_reactions', 'potential', 'wage_eur', 'release_clause_eur', 'value_eur', 'passing', 'dribbling']\n",
    "no_release_clause = players_19[pd.isna(players_19['release_clause_eur'])].iloc[0:5]\n",
    "players_19 = players_19[features]\n",
    "no_release_clause = no_release_clause[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4edfbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_19['release_clause_eur'] = players_19['release_clause_eur'].apply(lambda x: '{:.0f}'.format(x))\n",
    "players_19['value_eur'] = players_19['value_eur'].apply(lambda x: '{:.0f}'.format(x))\n",
    "players_19['wage_eur'] = players_19['wage_eur'].apply(lambda x: '{:.0f}'.format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6f1923",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_19 = pd.concat([\n",
    "    players_19.iloc[0:5],\n",
    "    players_19.iloc[9000:9005],\n",
    "    players_19.iloc[18080:18085],\n",
    "    no_release_clause\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe23bcb4",
   "metadata": {},
   "source": [
    "Dataframe for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23bd050",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2258972e",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1]Jim Frost. 2018. Interpreting Correlation Coefficients. Statistics By Jim. Retrieved June 16, 2024 from https://statisticsbyjim.com/basics/correlations/\n",
    "\n",
    "[2]Alboukadel Kassambara. 2018. Transform Data to Normal Distribution in R: Easy Guide. Datanovia. Retrieved June 17, 2024 from https://www.datanovia.com/en/lessons/transform-data-to-normal-distribution-in-r/#google_vignette\n",
    "\n",
    "[3]Stefano Leone. 2021. FIFA 22 complete player dataset. Kaggle.com. Retrieved June 22, 2024 from https://www.kaggle.com/datasets/stefanoleone992/fifa-22-complete-player-dataset?resource=download&select=players_19.csv\n",
    "\n",
    "[4]Stefano Leone. 2024. FIFA 23 complete player dataset. www.kaggle.com. Retrieved June 22, 2024 from https://www.kaggle.com/datasets/stefanoleone992/fifa-23-complete-player-dataset\n",
    "\n",
    "[5]Hrvoje Smolic. 2024. How Much Data Do You Need for Machine Learning. Graphic Note. Retrieved June 16, 2024 from https://graphite-note.com/how-much-data-is-needed-for-machine-learning/#:~:text=The%20rule%2Dof%2Dthumb%20rule\n",
    "\n",
    "[6]StackExchange. 2014. regression - What is the reason the log transformation is used with right-skewed distributions? Cross Validated. Retrieved June 17, 2024 from https://stats.stackexchange.com/questions/107610/what-is-the-reason-the-log-transformation-is-used-with-right-skewed-distribution\n",
    "\n",
    "[7]StackExchange. 2023. How many features is too many when using feature selection methods? Data Science Stack Exchange. Retrieved June 16, 2024 from https://datascience.stackexchange.com/questions/122640/how-many-features-is-too-many-when-using-feature-selection-methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
